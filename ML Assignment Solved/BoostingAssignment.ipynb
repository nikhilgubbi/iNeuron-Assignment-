{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.\tWhat is Boosting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is an ensemble approach(meaning it involves several trees) that starts from a weaker decision and keeps on building the models such that the final prediction is the weighted sum of all the weaker decision-makers.The weights are assigned based on the performance of an individual tree.\n",
    "\n",
    "Boosting is an iterative technique which adjust the weight of an observation based on the last classification. If an observation was classfied incorrectly, it tries to increase the weight of this observation and vice versa. Boosting in\n",
    "general decreases the bias error and builds strong predictive models. However,they may overfit on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.\tHow do boosting and bagging differ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging (stands for Bootstrap Aggregation) is the way decrease the variance of your prediction by generating additional data for training from your original data set using combinations with repetitions to produce multi sets of the same carnality/size as your original data. By increasing the size of your training set you can't improve the model predictive force, but just decrease the variance, narrowly tuning the prediction to expected outcome.\n",
    "\n",
    "Boosting is a an approach to calculate the output using several different models and then average the result using a weighted average approach. By combining the advantages and pitfalls of these approaches by varying your weighting formula you can come up with a good predictive force for a wider range of input data, using different narrowly tuned models.\n",
    "\n",
    "**Bagging:**\n",
    "\n",
    "parallel ensemble: each model is built independently\n",
    "\n",
    "aim to decrease variance, not bias\n",
    "\n",
    "suitable for high variance low bias models (complex models)\n",
    "\n",
    "an example of a tree based method is random forest, which develop fully grown trees (note that RF modifies the grown procedure to reduce the correlation between trees)\n",
    "\n",
    "**Boosting:**\n",
    "\n",
    "sequential ensemble: try to add new models that do well where previous models lack\n",
    "\n",
    "aim to decrease bias, not variance\n",
    "\n",
    "suitable for low variance high bias models\n",
    "\n",
    "an example of a tree based method is gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.\tWhat are weak and strong classifiers?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weak classifiers (or weak learners) are classifiers which perform only slightly better than a random classifier. These are thus classifiers which have some clue on how to predict the right labels, but not as much as strong classifiers have like, e.g., Naive Bayes, Neurel Networks or SVM.\n",
    "\n",
    "One of the simplest weak classifiers is the Decision Stump, which is a one-level Decision Tree. It selects a threshold for one feature and splits the data on that threshold. AdaBoost will then train an army of these Decision Stumps which each focus on one part of the characteristics of the data.\n",
    "\n",
    "A weak learner is defined to be a classifier that is only slightly correlated with the true classification. In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.\n",
    "\n",
    "A strong learner has an error rate closer to 0. To convert a weak learner into strong learner, we take a family of weak learners, combine them and vote. This turns this family of weak learners into strong learners. The idea here is that the family of weak learners should have a minimum correlation between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.\tWhy are trees deemed fit for boosting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to check with Sudhanshu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.\tExplain the step by step implementation of ADA Boost.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps to implement the Ada Boost algorithm using the decision trees are as follows:\n",
    "\n",
    "**Algorithm**:\n",
    "\n",
    "Assume that the number of training samples is denoted by $N$, and the number of iterations (created trees) is $M$. Notice that possible class outputs are $Y=\\{-1,1\\}$\n",
    "\n",
    "1. Initialize the observation weights  $w_i=\\frac{1}{N}$ where $i = 1,2, \\dots, N$ for all the samples.\n",
    "2. For $m=1$ to $M$:\n",
    "    - fit a classifier $G_m(x)$ to the training data using weights $w_i$,\n",
    "    - compute $err_m = \\frac{\\sum_{i=1}^{N} w_i I (y_i \\neq G_m(x))}{\\sum_{i=1}^{N}w_i}$,\n",
    "    - compute $\\alpha_m = \\frac {1}{2} \\log (\\frac{(1-err_m)}{err_m})$. This is the contribution of that tree to the final result.\n",
    "    - calculate the new weights using the formula:\n",
    "    \n",
    "    $w_i \\leftarrow w_i \\cdot \\exp [\\alpha_m \\cdot I (y_i \\neq G_m(x)]$, where $i = 1,2, \\dots, N$\n",
    "- Normalize the new sample  weights so that their sum is 1.\n",
    "- Construct the next tree using the new weights\n",
    "\n",
    "\n",
    " 3. At the end, compare the summation of results from all the trees and the final result is either the one with the highest sum(for regression) or it is the class which has the most weighted voted average(for classification).\n",
    "\n",
    "       Output $G_m(x) = argmax [\\sum_{m=1}^{M} \\alpha_m G_m(x)]$ (Regression)\n",
    "\n",
    "       Output $G_m(x) = sigm [\\sum_{m=1}^{M} \\alpha_m G_m(x)]$ (Classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.\tWhat are pseudo residuals?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo residual= actual label- the predicted result (which is average in the first iteration)\n",
    "  Mathematically,\n",
    "  \n",
    "     derivative of the pseudo residual=$(\\frac {\\delta L(y_i,f(x_i))}{\\delta (f(x_i))})$\n",
    "     \n",
    "     where, L is the loss function.\n",
    "                          \n",
    "               \n",
    "     Here, the gradient of the error term is getting calculated as the goal is to minimize the error. Hence the name gradient boosted trees\n",
    "- create a tree to predict the pseudo residuals instead  of a tree to predict for the actual column values.\n",
    "- new result= previous result+learning rate* residual \n",
    "   \n",
    "   Mathematically, \n",
    "     $ F_1(x)= F_0(x)+ \\nu \\sum \\gamma $\n",
    "     \n",
    " where  $ \\nu $ is the learning rate and $ \\gamma $ is the residual\n",
    "\n",
    "Repeat these steps until the residual stops decreasing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.\tExplain the step by step implementation of Gradient boosted trees.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Gradient Boosted Trees* use decision trees as estimators. It can work with different loss functions (regression, classification, risk modelling etc.), evaluate it's gradient and approximates it with a simple tree (stage-wisely, that minimizes the overall error).\n",
    "\n",
    "AdaBoost is a special case of Gradient Boosted Tree that uses exponential loss function.\n",
    "\n",
    "**The Algorithm:**\n",
    "\n",
    "- Calculate the average of the label column as initially this average shall minimise the total error.\n",
    "- Calculate the pseudo residuals.\n",
    "       Pseudo residual= actual label- the predicted result (which is average in the first iteration)\n",
    "  Mathematically,\n",
    "  \n",
    "     derivative of the pseudo residual=$(\\frac {\\delta L(y_i,f(x_i))}{\\delta (f(x_i))})$\n",
    "     \n",
    "     where, L is the loss function.\n",
    "                          \n",
    "               \n",
    "     Here, the gradient of the error term is getting calculated as the goal is to minimize the error. Hence the name gradient boosted trees\n",
    "- create a tree to predict the pseudo residuals instead  of a tree to predict for the actual column values.\n",
    "- new result= previous result+learning rate* residual \n",
    "   \n",
    "   Mathematically, \n",
    "     $ F_1(x)= F_0(x)+ \\nu \\sum \\gamma $\n",
    "     \n",
    " where  $ \\nu $ is the learning rate and $ \\gamma $ is the residual\n",
    "\n",
    "Repeat these steps until the residual stops decreasing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.\tExplain the step by step implementation of XGBoost Algorithm.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost improves the gradient boosting method even further.\n",
    "> **XGBoost** (*extreme gradient boosting*) regularises data better than normal gradient boosted Trees.\n",
    "\n",
    "It was developed by Tianqi Chen in C++ but now has interfaces for Python, R, Julia.\n",
    "\n",
    "XGBoost's objective function is the sum of loss function evaluated over all the predictions and a regularisation function for all predictors ($j$ trees). In the formula $f_j$ means a prediction coming from the $j^th$ tree.\n",
    "\n",
    "$$\n",
    "obj(\\theta) = \\sum_{i}^{n} l(y_i - \\hat{y_i}) +  \\sum_{j=1}^{j} \\Omega (f_j)\n",
    "$$\n",
    "\n",
    "Loss function depends on the task being performed (classification, regression, etc.) and a regularization term is described by the following equation:\n",
    "\n",
    "$$\n",
    "\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T}w_j^2\n",
    "$$\n",
    "\n",
    "First part ($\\gamma T$) is responsible for controlling the overall number of created leaves, and the second term ($\\frac{1}{2} \\lambda \\sum_{j=1}^{T}w_j^2$) watches over the scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9.\tWhat are the advantages of XGBoost?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is an efficient and easy to use algorithm which delivers high performance and accuracy as compared to other algorithms. XGBoost is also known as regularized version of GBM. Let see some of the advantages of XGBoost algorithm:\n",
    "\n",
    "**1. Regularization:** XGBoost has in-built L1 (Lasso Regression) and L2 (Ridge Regression) regularization which prevents the model from overfitting. That is why, XGBoost is also called regularized form of GBM (Gradient Boosting Machine).\n",
    "\n",
    "While using Scikit Learn libarary, we pass two hyper-parameters (alpha and lambda) to XGBoost related to regularization. alpha is used for L1 regularization and lambda is used for L2 regularization.\n",
    "\n",
    "**2. Parallel Processing:** XGBoost utilizes the power of parallel processing and that is why it is much faster than GBM. It uses multiple CPU cores to execute the model.\n",
    "\n",
    "While using Scikit Learn libarary, nthread hyper-parameter is used for parallel processing. nthread represents number of CPU cores to be used. If you want to use all the available cores, don't mention any value for nthread and the algorithm will detect automatically.\n",
    "\n",
    "**3. Handling Missing Values:** XGBoost has an in-built capability to handle missing values. When XGBoost encounters a missing value at a node, it tries both the left and right hand split and learns the way leading to higher loss for each node. It then does the same when working on the testing data.\n",
    "\n",
    "**4. Cross Validation:** XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run. This is unlike GBM where we have to run a grid-search and only a limited values can be tested.\n",
    "\n",
    "**5. Effective Tree Pruning:** A GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm. XGBoost on the other hand make splits upto the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain.\n",
    "\n",
    "For example: There may be a situation where split of negative loss say -4 may be followed by a split of positive loss +13. GBM would stop as it encounters -4. But XGBoost will go deeper and it will see a combined effect of +9 of the split and keep both."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
