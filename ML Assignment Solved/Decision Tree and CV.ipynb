{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)\tHow decision tree works for a regression problem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree is one of the most fundamental algorithms for classification and regression in the Machine Learning world.\n",
    "\n",
    "The most common method for constructing regression tree is CART (Classification and Regression Tree) methodology, which is also known as recursive partitioning.\n",
    "\n",
    "Take basic regression tree as example:\n",
    "\n",
    "The method starts by searching for every distinct values of all its predictors, and splitting the value of a predictor that minimizes the following statistic (other regression tree models have different optimization criteria):\n",
    "\n",
    "SSE=∑i∈S1(yi−y1^)+∑i∈S2(yi−y2^)\n",
    "\n",
    "where y1^ and y2^ are the average values of the dependent variable in groups S1 and S2.\n",
    "\n",
    "For groups S1 and S2, the method will recursively split the predictor values within groups. In practice, the method stops when the sample size of the split group falls below certain threshold, e.g., 50.\n",
    "\n",
    "To prevent over-fitting, the constructed tree can be pruned by penalizing the SSE with tree size:\n",
    "\n",
    "SSEcp=SSE+(cp)*(St)\n",
    "\n",
    "where St is the size of the tree (number of terminal nodes), and cp is complexity parameter. Smaller cp will lead to larger trees, and vice versa. Of course, this parameter can also be tuned by cross-validation.\n",
    "\n",
    "Unlike linear regression models that calculate the coefficients of predictors, tree regression models calculate the relative importance of predictors. The relative importance of predictors can be computed by summing up the overall reduction of optimization criteria like SSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)\tWhy Recursive binary splitting is called Greedy Approach?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a binary decision tree is actually a process of dividing up the input space. A greedy approach is used to divide the space called recursive binary splitting.\n",
    "\n",
    "This is a numerical procedure where all the values are lined up and different split points are tried and tested using a cost function. The split with the best cost (lowest cost because we minimize cost) is selected.\n",
    "\n",
    "All input variables and all possible split points are evaluated and chosen in a greedy manner (e.g. the very best split point is chosen each time).\n",
    "\n",
    "For regression predictive modeling problems the cost function that is minimized to choose split points is the sum squared error across all training samples that fall within the rectangle:\n",
    "\n",
    "sum(y – prediction)^2\n",
    "\n",
    "Where y is the output for the training sample and prediction is the predicted output for the rectangle.\n",
    "\n",
    "For classification the Gini index function is used which provides an indication of how “pure” the leaf nodes are (how mixed the training data assigned to each node is).\n",
    "\n",
    "G = sum(pk * (1 – pk))\n",
    "\n",
    "Where G is the Gini index over all classes, pk are the proportion of training instances with class k in the rectangle of interest. A node that has all classes of the same type (perfect class purity) will have G=0, where as a G that has a 50-50 split of classes for a binary classification problem (worst purity) will have a G=0.5.\n",
    "\n",
    "For a binary classification problem, this can be re-written as:\n",
    "\n",
    "G = 2 * p1 * p2\n",
    "or\n",
    "G = 1 – (p1^2 + p2^2)\n",
    "\n",
    "The Gini index calculation for each node is weighted by the total number of instances in the parent node. The Gini score for a chosen split point in a binary classification problem is therefore calculated as follows:\n",
    "\n",
    "G = ((1 – (g1_1^2 + g1_2^2)) * (ng1/n)) + ((1 – (g2_1^2 + g2_2^2)) * (ng2/n))\n",
    "\n",
    "Where G is the Gini index for the split point, g1_1 is the proportion of instances in group 1 for class 1, g1_2 for class 2, g2_1 for group 2 and class 1, g2_2 group 2 class 2, ng1 and ng2 are the total number of instances in group 1 and 2 and n are the total number of instances we are trying to group from the parent node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)\tWhat do you understand by Greedy approach?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A greedy algorithm is an algorithmic strategy that makes the best optimal choice at each small stage with the goal of this eventually leading to a globally optimum solution. This means that the algorithm picks the best solution at the moment without regard for consequences. It picks the best immediate output, but does not consider the big picture, hence it is considered greedy.\n",
    "\n",
    "A greedy algorithm works by choosing the best possible answer in each step and then moving on to the next step until it reaches the end, without regard for the overall solution. It only hopes that the path it takes is the globally optimum one, but as proven time and again, this method does not often come up with a globally optimum solution. In fact, it is entirely possible that the most optimal short-term solutions lead to the worst possible global outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)\tWhat is Pruning?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we remove sub-nodes of a decision node, this procsss is called pruning or\n",
    "opposite process of splitting.\n",
    "\n",
    "According to wikipedia : Pruning is a technique in machine learning and search algorithms that reduces the size of decision trees by removing sections of the tree that provide little power to classify instances. Pruning reduces the complexity of the final classifier, and hence improves predictive accuracy by the reduction of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5)\tWhat’s the difference between pre pruning and post pruning?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-pruning\n",
    "\n",
    "Post-pruning, also known as backward pruning, is the process where the decision tree is generated first and then the non-significant branches are removed. Cross-validation set of data is used to check the effect of pruning and tests whether expanding a node will make an improvement or not. If any improvement is there then we continue by expanding that node else if there is reduction in accuracy then the node not be expanded and should be converted in a leaf node.\n",
    "\n",
    "\n",
    "#### Pre-pruning\n",
    "\n",
    "Pre-pruning, also known as forward pruning, stops the non-significant branches from generating. It uses a condition to decide when should it terminate splitting of some of the branches prematurely as the tree is generated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6)\tWhat is Entropy? How is it calculated?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is defined as the quantitative measure of disorder or randomness in a system. Entropy is a measure of disorder or uncertainty and the goal of machine learning models and Data Scientists in general is to reduce uncertainty.\n",
    "\n",
    ">>**Entropy is the measure of randomness in the data. In other words, it gives the impurity present in the dataset.**\n",
    "\n",
    "When we split our nodes into two regions and put different observations in both the regions, the main goal is to reduce the entropy i.e. reduce the randomness in the region and divide our data cleanly than it was in the previous node. If splitting the node doesn’t lead into entropy reduction, we try to split based on a different condition, or we stop. \n",
    "A region is clean (low entropy) when it contains data with the same labels and random if there is a mixture of labels present (high entropy).\n",
    "Let’s suppose there are ‘m’ observations and we need to classify them into categories 1 and 2.\n",
    "Let’s say that category 1 has ‘n’ observations and category 2 has ‘m-n’ observations.\n",
    "\n",
    "p= n/m  and    q = m-n/m = 1-p\n",
    "\n",
    "then, entropy for the given set is:\n",
    "\n",
    "\n",
    "          E = -p*log2(p) – q*log2(q) \n",
    "           \n",
    "           \n",
    "When all the observations belong to category 1, then p = 1 and all observations belong to category 2, then p =0, int both cases E =0, as there is no randomness in the categories.\n",
    "If half of the observations are in category 1 and another half in category 2, then p =1/2 and q =1/2, and the entropy is maximum, E =1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7)\tWhat is Gini Impurity?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to wikipedia, ‘Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labelled if it was randomly labelled according to the distribution of labels in the subset.’ It is calculated by multiplying the probability that a given observation is classified into the correct class and sum of all the probabilities when that particular observation is classified into the wrong class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8)\tWhat do you understand by Information Gain? How does it help in tree building?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Information Gain is based on the decrease in entropy after a dataset is split on an attribute. Constructing a decision tree is all about finding attributes that returns the highest information gain.\n",
    "\n",
    "**How to Find the Information Gain**\n",
    "\n",
    "Please apply the entropy (Mathematical Formulae) to calculate Information\n",
    "Gain. Gain (T,X) = Entropy(T) – Entropy(T,X) here represent target variable and\n",
    "X represent features\n",
    "\n",
    "suppose that one is building a decision tree for some data describing the customers of a business. Information gain is often used to decide which of the attributes are the most relevant, so they can be tested near the root of the tree. One of the input attributes might be the customer's credit card number. This attribute has a high mutual information, because it uniquely identifies each customer, but we do not want to include it in the decision tree: deciding how to treat a customer based on their credit card number is unlikely to generalize to customers we haven't seen before (overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9)\tHow does node selection take place while building a tree?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes).\n",
    "\n",
    "**Step 1:**- How to find the Root Node\n",
    "\n",
    "Use Information gain to understand the each attribute information w.r.t target\n",
    "variable and place the attribute with the highest information gain as root node.\n",
    "\n",
    "**Step 2:**- How to Find the Information Gain\n",
    "\n",
    "Please apply the entropy (Mathematical Formulae) to calculate Information\n",
    "Gain. Gain (T,X) = Entropy(T) – Entropy(T,X) here represent target variable and\n",
    "X represent features.\n",
    "\n",
    "**Step3:** Identification of Terminal Node\n",
    "\n",
    "Based on the information gain value obtained from the above steps, identify the\n",
    "second most highest information gain and place it as the terminal node.\n",
    "\n",
    "**Step 4:** Predicted Outcome\n",
    "\n",
    "Recursively iterate the step4 till we obtain the leaf node which would be our\n",
    "predicted target variable.\n",
    "\n",
    "**Step 5:** Tree Pruning and optimization for good results\n",
    "\n",
    "It helps to reduce the size of decision trees by removing sections of the tree to\n",
    "avoid over fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10)\tWhat are different algorithms available for decision tree?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different Algorithms for Decision Tree**\n",
    "\n",
    "**ID3 (Iterative Dichotomiser) :** It is one of the algorithms used to construct decision tree for classification. It uses Information gain as the criteria for finding the root nodes and splitting them. It only accepts categorical attributes.\n",
    "\n",
    "**C4.5 :** It is an extension of ID3 algorithm, and better than ID3 as it deals both continuous and discreet values.It is also used for classfication purposes.\n",
    "\n",
    "**Classfication and Regression Algorithm(CART) :** It is the most popular algorithm used for constructing decison trees. It uses ginni impurity as the default calculation for selecting root nodes, however one can use \"entropy\" for criteria as well. This algorithm works on both regression as well as classfication problems. We will use this algorithm in our pyhton implementation. \n",
    "\n",
    "Entropy and Ginni impurity can be used reversibly. It doesn't affects the result much. Although, ginni is easier to compute than entropy, since entropy has a log term calculation. That's why CART algorithm uses ginni as the default algorithm.\n",
    "If we plot ginni vs entropy graph, we can see there is not much difference between them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11)\tWhat’s the main difference between Gini Impurity and Entropy on the basis of computation time?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12)\tWhat are the disadvantages and advantages of using a Decision Tree?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Advantages of Decision Tree:\n",
    "\n",
    "   * It can be used for both Regression and Classification problems.\n",
    "   * Decision Trees are very easy to grasp as the rules of splitting is clearly mentioned.\n",
    "   * Complex decision tree models are very simple when visualized. It can be understood just by visualising.\n",
    "   * Scaling and normalization are not needed.\n",
    "\n",
    "\n",
    "##### Disadvantages of Decision Tree:\n",
    "\n",
    "\n",
    "   * A small change in data can cause instability in the model because of the greedy approach.\n",
    "   * Probability of overfitting is very high for Decision Trees.\n",
    "   * It takes more time to train a decision tree model than other classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**13)\tHow do you deploy model in Heroku?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-requisites for cloud deployment:**\n",
    "\n",
    "Basic knowledge of flask framework.\n",
    "Any Python IDE installed(we are using PyCharm).\n",
    "A Heroku account.\n",
    "Basic understanding of HTML.\n",
    "\n",
    "**Deployment to Heroku:**\n",
    "\n",
    "After installing the Heroku CLI, Open a command prompt window and navigate to your project folder.\n",
    "Type the command heroku login to login to your heroku account.\n",
    "After logging in to Heroku, enter the command heroku create to create a heroku app. It will give you the URL of your Heroku app after successful creation. Or alternatively, you can go to the heroku website and create an app directly.\n",
    "Before deploying the code to the Heroku cloud, we need to commit the changes to the git repository.\n",
    "Type the command git init to initialize a local git repository.\n",
    "Enter the command git status to see the uncommitted changes.\n",
    "Enter the command git add . to add the uncommitted changes to the local repository.\n",
    "Enter the command git commit -am \"make it better\" to commit the changes to the local repository.\n",
    "Enter the command git push heroku master to push the code to the heroku cloud.\n",
    "After deployment, heroku gives you the URL to hit the web API.\n",
    "Once your application is deployed successfully, enter the command heroku logs --tail to see the logs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**14)\tWhat challenges you faced while deploying the model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one time i got the issue while pushing the code through git, second time it took, finally we are able to deploy the code which iNeuron has provided , "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)\tWhat is Cross validation?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a model validation technique for evaluating how the outcomes of a statistical analysis will generalize to an independent data set. Mainly used in backgrounds where the objective is forecast and one wants to estimate how accurately a model will accomplish in practice.The goal of cross-validation is to term a data set to test the model in the training phase (i.e. validation data set) in order to limit problems like over fitting, and get an insight on how the model will generalize to an independent data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)\tWhy do we need to implement Cross validation?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are so many reason to implement cross validation in our data:\n",
    "\n",
    "**1. Use All Your Data :**\n",
    "\n",
    "When we have very little data, splitting it into training and test set might leave us with a very small test set. Say we have only 100 examples, if we do a simple 80–20 split, we’ll get 20 examples in our test set. It is not enough. We can get almost any performance on this set only due to chance. The problem is even worse when we have a multi-class problem. If we have 10 classes and only 20 examples, It leaves us with only 2 examples for each class on average. Testing anything on only 2 examples can’t lead to any real conclusion.\n",
    "\n",
    "If we use cross-validation in this case, we build K different models, so we are able to make predictions on all of our data. For each instance, we make a prediction by a model that didn’t see this example, and so we are getting 100 examples in our test set. For the multi-class problem, we get 10 examples for each class on average, and it’s much better than just 2. After we evaluated our learning algorithm (see #2 below) we are now can train our model on all our data because if our 5 models had similar performance using different train sets, we assume that by training it on all the data will get similar performance.\n",
    "By doing cross-validation, we’re able to use all our 100 examples both for training and for testing while evaluating our learning algorithm on examples it has never seen before.\n",
    "\n",
    "**2. Get More Metrics:**\n",
    "\n",
    "As mentioned in #1, when we create five different models using our learning algorithm and test it on five different test sets, we can be more confident in our algorithm performance. When we do a single evaluation on our test set, we get only one result. This result may be because of chance or a biased test set for some reason. By training five (or ten) different models we can understand better what’s going on. Say we trained five models and we use accuracy as our measurement. We could end up in several different situations. The best scenario is that our accuracy is similar in all our folds, say 92.0, 91.5, 92.0, 92.5 and 91.8. This means that our algorithm (and our data) is consistent and we can be confident that by training it on all the data set and deploy it in production will lead to similar performance.\n",
    "\n",
    "However, we could end up in a slightly different scenario, say 92.0, 44.0, 91.5, 92.5 and 91.8. These results look very strange. It looks like one of our folds is from a different distribution, we have to go back and make sure that our data is what we think it is.\n",
    "\n",
    "The worst scenario we can end up in is when we have considerable variation in our results, say 80, 44, 99, 60 and 87. Here it looks like that our algorithm or our data (or both) is no consistent, it could be that our algorithm is unable to learn, or our data is very complicated.\n",
    "\n",
    "By using Cross-Validation, we are able to get more metrics and draw important conclusion both about our algorithm and our data.\n",
    "\n",
    "**3. Use Models Stacking:**\n",
    "\n",
    "When we have limited data (as in most cases), we can’t really do it. Also, we can’t train both our models on the same dataset because then, our second model learns on predictions that our first model already seen. These will probably be over-fitted or at least have better results than on a different set. This means that our second algorithm is trained not on what it will be tested on. This may lead to different effects in our final evaluations that will be hard to understand.\n",
    "\n",
    "By using cross-validation, we can make predictions on our dataset in the same way as described before and so our second’s models input will be real predictions on data that our first model never seen before.\n",
    "\n",
    "**4. Work with Dependent/Grouped Data:**\n",
    "\n",
    "When we perform a random train-test split of our data, we assume that our examples are independent. That means that by knowing/seeing some instance will not help us understand other instances. However, that’s not always the case.\n",
    "\n",
    "Consider a speech recognition system. Our data may include different speakers saying different words. Let’s look at spoken digits recognition. In this dataset, for example, there are 3 speakers and 1500 recordings (500 for each speaker). If we do a random split, our training and test set will share the same speaker saying the same words! This is, of course, will boost our algorithm performance but once tested on a new speaker, our results will be much worse.\n",
    "\n",
    "The proper way to do it is to split the speakers, i.e., use 2 speakers for training and use the third for testing. However, then we’ll test our algorithm only on one speaker. It is not enough. We need to know how our algorithm performs on different speakers.\n",
    "\n",
    "We can use cross-validation on the speakers level. We will train 3 models, each time using one speaker for testing and two others for training. This way we’ll be able to evaluate better our algorithm (as described above) and finally build our model on all speakers.\n",
    "\n",
    "**5. Parameters Fine-Tuning:**\n",
    "\n",
    "This is one of the most common and obvious reasons to do cross validation. Most of the learning algorithms require some parameters tuning. It could be the number of trees in Gradient Boosting classifier, hidden layer size or activation functions in a Neural Network, type of kernel in an SVM and many more. We want to find the best parameters for our problem. We do it by trying different values and choosing the best ones. There are many methods to do this. It could be a manual search, a grid search or some more sophisticated optimization. However, in all those cases we can’t do it on our training test and not on our test set of course. We have to use a third set, a validation set.\n",
    "\n",
    "By splitting our data into three sets instead of two, we’ll tackle all the same issues we talked about before, especially if we don’t have a lot of data. By doing cross-validation, we’re able to do all those steps using a single set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)\tWhat are different types of CV methods?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hold Out Method:**\n",
    "\n",
    "It is the most basic of the CV techniques. It simply divides the dataset into two sets of training and test. The training dataset is used to train the model and then test data is fitted in the trained model to make predictions. We check the accuracy and assess our model on that basis. This method is used as it is computationally less costly. But the evaluation based on the Hold-out set can have a high variance because it depends heavily on which data points end up in the training set and which in test data. The evaluation will be different every time this division changes.\n",
    "\n",
    "**K-Fold Cross Validation:**\n",
    "\n",
    "As there is never enough data to train your model, removing a part of it for validation poses a problem of underfitting. By reducing the training data, we risk losing important patterns/ trends in data set, which in turn increases error induced by bias. So, what we require is a method that provides ample data for training the model and also leaves ample data for validation. K Fold cross validation does exactly that.\n",
    "\n",
    "In K Fold cross validation, the data is divided into k subsets. Now the holdout method is repeated k times, such that each time, one of the k subsets is used as the test set/ validation set and the other k-1 subsets are put together to form a training set. \n",
    "\n",
    "**Leave One Out Cross Validation (LOOCV):**\n",
    "\n",
    "LOOCV is a special case of k-fold CV, where k becomes equal to n (number of observations). So instead of creating two subsets, it selects a single observation as a test data and rest of data as the training data. The error is calculated for this test observations. Now, the second observation is selected as test data, and the rest of the data is used as the training set. Again, the error is calculated for this particular test observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)\tHow bias and variance varies for each CV method?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A k-fold CV with k < n has a computational advantage to LOOCV. But putting computational issues aside, a less obvious but potentially more important advantage of k-fold CV is that it often gives more accurate estimates of the test error rate than does LOOCV. The validation set approach can lead to overestimates of the test error rate since in this approach the the training set used to fit the statistical learning method contains only half the observations of the entire data set. Using this logic, it is not hard to see that LOOCV will give approximately unbiased estimates of the test error since each training set contains n − 1 observations, which is almost as many as the number of observations in the full data set. And performing k-fold CV for, say, k = 5 or k = 10 will lead to an intermediate level of bias since each training set contains (k − 1)n/k observations—fewer than in the LOOCV approach, but substantially more than in the validation set approach. Therefore, from the perspective of bias reduction, it is clear that LOOCV is to be preferred to k-fold CV. However, we know that bias is not the only source for concern in an estimating procedure; we must also consider the procedure’s variance. It turns out that LOOCV has higher variance than does k-fold CV with k < n. Why is this the case? When we perform LOOCV, we are in effect averaging the outputs of n fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other. In contrast, when we perform k-fold CV with k < n, we are averaging the outputs of k fitted models that are somewhat less correlated with each other since the overlap between the training sets in each model is smaller. Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from k-fold CV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5)\tIs Train Test Split a kind of CV? True or False.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes Its True,  Cross-validation is a resampling technique with a basic idea of dividing the training dataset into two parts i.e. train and test. On one part(train) you try to train the model and on the second part(test) i.e. the data which is unseen for the model, you make the prediction and check how well your model works on it. If the model works with good accuracy on your test data, it means that the model has not overfitted the training data and can be trusted with the prediction, whereas if it performs with bad accuracy then our model is not to be trusted and we need to tweak our algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6)\tHow can we check over fitting using CV?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check for hints of overfitting by using a training set and a test set (or a training, validation and test set). As others have mentioned, you can either split the data into training and test sets, or use cross-fold validation to get a more accurate assessment of your classifier's performance.\n",
    "\n",
    "Since your dataset is small, splitting your data into training and test sets isn't recommended. Use cross validation.\n",
    "\n",
    "This can be done using either the cross_validate or cross_val_score function; the latter providing multiple metrics for evaluation. In addition to test scores the latter also provides fit times and score times."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
