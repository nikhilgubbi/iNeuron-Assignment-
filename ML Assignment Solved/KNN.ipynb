{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)\tExplain the working of KNN algorithm.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k-nearest neighbors (KNN) algorithm is a simple, supervised machine learning algorithm that can be used to solve both classification and regression problems. It’s easy to implement and understand, but has a major drawback of becoming significantly slows as the size of that data in use grows.\n",
    "\n",
    "KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression.In the case of classification and regression, we saw that choosing the right K for our data is done by trying several Ks and picking the one that works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)\tHow KNN predicts the output for regression and classification problems?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-nearest neighbors (KNN) is a type of supervised learning algorithm which is used for both regression and classification purposes, but mostly it is used for the later. Given a dataset with different classes, KNN tries to predict the correct class of test data by calculating the distance between the test data and all the training points. It then selects the k points which are closest to the test data. Once the points are selected, the algorithm calculates the probability (in case of classification) of the test point belonging to the classes of the k training points and the class with the highest probability is selected. In the case of a regression problem, the predicted value is the mean of the k selected training points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)\tWhat are the different distances used in KNN? How are they calculated?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance: \n",
    "\n",
    "It is the most commonly used method to calculate the distance between two points.\n",
    "The Euclidean distance between two points ‘p(p1,p2)’ and ‘q(q1,q2)’ is calculated  as :\n",
    "\n",
    "<img src=\"4.png\" width=\"\">       image source : Wikipedia\n",
    "\n",
    "<img src=\"5.png\" width=\"\">\n",
    "\n",
    "                                          \n",
    "Similarly,for n-dimensional space, the Euclidean distance is given as :\n",
    "\n",
    "<img src=\"6.png\" width=\"\">\n",
    " \n",
    "### Hamming distance\n",
    "A/c to Wikipedia, hamming distance is a distance metric that measures the number of mismatches between two vectors. It is mostly used in the case of categorical data.\n",
    "\n",
    "<img src=\"7.png\" width=\"\">\n",
    "                                                               \n",
    "Generally, if we have features as categorical data then we consider the difference to be 0 if both the values are the same and the difference is 1 if both the values are different.\n",
    "\n",
    "### Manhattan Distance\n",
    "A/c to Wikipedia, The Manhattan distance, also known as L1 norm, Taxicab norm, Rectilinear distance or City block distance. This distance represents the sum of the absolute differences between the opposite values in vectors.\n",
    "\n",
    "<img src=\"8.png\" width=\"\">\n",
    "                                                           \n",
    "Manhattan Distance is less influenced by outliers than the Euclidean distance. With very high dimensional data it is more preferred. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)\tWhat are Lazy Learners? Why KNN is called a lazy learner?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-NN algorithms are often termed as Lazy learners. Let’s understand why is that. Most of the algorithms like Bayesian classification, logistic regression, SVM etc., are called Eager learners. These algorithms generalize over the training set before receiving the test data i.e. they create a model based on the training data before receiving the test data and then do the prediction/classification on the test data. But this is not the case with the k-NN algorithm. It doesn’t create a generalized model for the training set but waits for the test data. Once test data is provided then only it starts generalizing the training data to classify the test data. So, a lazy learner just stores the training data and waits for the test set. Such algorithms work less while training and more while classifying a given test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5)\tHow do we select the value of k? How bias and variance varies with k?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of k affects the k-NN classifier drastically. The flexibility of the model decreases with the increase of ‘k’.  With lower value of ‘k’ variance is high and bias is low but as we increase the value of ‘k’ variance starts decreasing and bias starts increasing. With very low values of ‘k’ there is a chance of algorithm overfitting the data whereas with very high value of ‘k’ there is a chance of underfitting. \n",
    "Let’s visualize the trade-off between ‘1/k’, train error rate and test error rate:\n",
    "\n",
    "<img src=\"9.png\" width=\"\">  image source: “ISLR”\n",
    "\n",
    "We can clearly see that the train error rate increases with the increase in the value of ‘k’ whereas test error rate decreases initially and then increases again.  So, our goal should be to choose such value of ‘k’ for which we get a minimum of both the errors and avoid overfitting as well as underfitting.\n",
    "We use different ways to calculate the optimum value of ‘k’ such as cross validation, error versus k curve, checking accuracy for each value of ‘k’ etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6)\tWhat are advantages and disadvantages of KNN?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros:**\n",
    "\n",
    "It can be used for both regression and classification problems.\n",
    "It is very simple and easy to implement.\n",
    "Mathematics behind the algorithm is easy to understand.\n",
    "There is no need to create model or do hyperparameter tuning.\n",
    "KNN doesn't make any assumption for the distribution of the given data.\n",
    "There is not much time cost in training phase.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "Finding the optimum value of ‘k’\n",
    "It takes a lot of time to compute the distance between each test sample and all training samples.\n",
    "Since the model is not saved beforehand in this algorithm (lazy learner), so every time one predicts a test value, it follows the same steps again and again. \n",
    "Since, we need to store the whole training set for every test set, it requires a lot of space.\n",
    "It is not suitable for high dimensional data.\n",
    "Expensive in testing phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7)\tDiscuss kDTree algorithm used for KNN.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-d tree is a hierarchical binary tree. When this algorithm is used for k-NN classficaition, it rearranges the whole dataset in a binary tree structure, so that when test data is provided, it would give out the result by traversing through the tree, which takes less time than brute search.\n",
    "\n",
    "<img src=\"10.png\" width=\"\"> \n",
    " \n",
    "The dataset is divided like a tree as shown in the above figure. Say we have 3 dimensional data i.e. (x,y,z) then the tree is formed with root node being one of the dimensions, here we start with ‘x’. Then on the next level the split is done on basis of the second dimension, ‘y’ in our case. Similarly, third level with 3rd dimension and so on.  And in case of ‘k’ dimensions, each split is made on basis of ‘k’ dimensions. \n",
    "Let’s understand how k-d trees are formed with an example:\n",
    "\n",
    "<img src=\"11.png\" width=\"\"> \n",
    "\n",
    "<img src=\"12.png\" width=\"\"> \n",
    "\n",
    "<img src=\"13.png\" width=\"\"> \n",
    "\n",
    "<img src=\"14.png\" width=\"\"> \n",
    " \n",
    " \n",
    "\n",
    "Once the tree is formed , it is easy for algorithm to search for the probable nearest neighbor just by traversing the tree.  The main problem k-d trees is that it gives probable nearest neighbors but can miss out actual nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**8)\tDiscuss Ball Tree algorithm used for KNN.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to k-d trees, Ball trees are also hierarchical data structure. These are very efficient specially in case of higher dimensions.\n",
    "\n",
    "These are formed by following steps:\n",
    "*\tTwo clusters are created initially\n",
    "*\tAll the data points must belong to atleast one of the clusters.\n",
    "* 3)\tOne point cannot be in both clusters.\n",
    "*\tDistance of the point is calculated from the centroid of the each cluster. The point closer to the centroid goes into that particular cluster.\n",
    "*\tEach cluster is then divided into sub clusters again, and then the points are classified into each cluster on the basis of distance from centroid.\n",
    "*\tThis is how the clusters are kept to be divided till a certain depth.\n",
    "\n",
    "\n",
    "<img src=\"15.png\" width=\"\"> \n",
    "                                                           \n",
    "\n",
    "Ball tree formation initially takes a lot of time but once the nested clusters are created, finding nearest neighbors is easier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
