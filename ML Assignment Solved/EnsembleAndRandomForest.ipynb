{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)\tWhat do you understand by Ensemble technique?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble technique is one of the most fundamental algorithms for classification and regression in the Machine Learning world.\n",
    "\n",
    "We regularly come across many game shows on television and you must have noticed an option of “Audience Poll”. Most of the times a contestant goes with the option which has the highest vote from the audience and most of the times they win. We can generalize this in real life as well where taking opinions from a majority of people is much more preferred than the opinion of a single person.\n",
    "Ensemble technique has a similar underlying idea where we aggregate predictions from a group of predictors, which may be classifiers or regressors, and most of the times the prediction is better than the one obtained using a single predictor. Such algorithms are called Ensemble methods and such predictors are called Ensembles.\n",
    "\n",
    "Let’s suppose we have ‘n’ predictors:\n",
    "\n",
    "Z1, Z2, Z3, ......., Zn with a standard deviation of σ\n",
    "\n",
    "Var(z) = σ^2\n",
    "\n",
    "If we use single predictors Z1, Z2, Z3, ......., Zn the variance associated with each will be σ2 but the expected value will be the average of all the predictors.\n",
    "\n",
    "Let’s consider the average of the predictors:\n",
    "\n",
    "µ = (Z1 + Z2 + Z3+.......+ Zn)/n\n",
    "\n",
    "if we use µ as the predictor then the expected value still remains the same but see the variance now:\n",
    "\n",
    "variance(µ) = σ^2/n\n",
    "\n",
    "So, the expected value remained ‘µ’ but variance decreases when we use average of all the predictors.\n",
    "\n",
    "This is why taking mean is preferred over using single predictors.\n",
    "\n",
    "Ensemble methods take multiple small models and combine their predictions to obtain a more powerful predictive power.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)\tExplain the idea behind ensemble techniques.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble learning helps improve machine learning results by combining several models. This approach allows the production of better predictive performance compared to a single model. That is why ensemble methods placed first in many prestigious machine learning competitions, such as the Netflix Competition, KDD 2009, and Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)\tWhat is Bootstrapping? How is sampling done in bootstrapping?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real life scenarios we don’t have multiple different training sets on which we can train our model separately and at the end combine their result. Here, bootstrapping comes into picture. Bootstrapping is a technique of sampling different sets of data from a given training set by using replacement. After bootstrapping the training dataset, we train model on all the different sets and aggregate the result. This technique is known as Bootstrap Aggregation or Bagging.\n",
    "\n",
    "The bootstrap method is a statistical technique for estimating quantities about a population by averaging estimates from multiple small data samples. Importantly, samples are constructed by drawing observations from a large data sample one at a time and returning them to the data sample after they have been chosen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)\tWhat is bagging?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging stands for bootstrap aggregation. One way to reduce the variance of an estimate is to average together multiple estimates. For example, we can train M different trees on different subsets of the data (chosen randomly with replacement).\n",
    "\n",
    "Bagging uses bootstrap sampling to obtain the data subsets for training the base learners. For aggregating the outputs of base learners, bagging uses voting for classification and averaging for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5)\tHow prediction is made in Bagging?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging is the type of ensemble technique in which a single training algorithm is used on different subsets of the training data where the subset sampling is done with replacement (bootstrap). \n",
    "Once the algorithm is trained on all the subsets, then bagging makes the prediction by aggregating all the predictions made by the algorithm on different subsets. In case of regression, bagging prediction is simply the mean of all the predictions and in the case of classifier, bagging prediction is the most frequent prediction (majority vote) among all the predictions.\n",
    "\n",
    "Bagging is also known as parallel model since we run all models parallely and combine there results at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6)\tHow Ensemble technique solves the high variance issue with Decision trees?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hold Out Method: \n",
    "\n",
    "It is the most basic of the CV techniques. It simply divides the dataset into two sets of training and test. The training dataset is used to train the model and then test data is fitted in the trained model to make predictions. We check the accuracy and assess our model on that basis. This method is used as it is computationally less costly. But the evaluation based on the Hold-out set can have a high variance because it depends heavily on which data points end up in the training set and which in test data. The evaluation will be different every time this division changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7)\tWhat is pasting? How is it different from bagging?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasting is an ensemble technique similar to bagging with the only difference being that there is no replacement done while sampling the training dataset. This causes less diversity in the sampled datasets and data ends up being correlated. That's why bagging is more preffered than pasting in real scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8)\tWhat is Out Of Bag evaluation?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In bagging, when different samples are collected, no sample contains all the data but a fraction of the original dataset.\n",
    "There might be some data which are never sampled at all. The remaining data which are not sampled are called out of bag instances. Since the model never trains over these data, they can be used for evaluating the accuracy of the model by using these data for predicition. We do not need validation set or cross validation and can use out of bag instances for that purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9)\tHow does a Random Forest model works?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest is a versatile machine learning method capable of performing both regression and classification tasks. It is also used for dimentionality reduction, treats missing values, outlier values. It is a type of ensemble learning method, where a group of weak models combine to form a powerful model.In Random Forest, we grow multiple trees as opposed to a single tree. To\n",
    "classify a new object based on attributes, each tree gives a classification. The forest chooses the classification having the most votes(Over all the trees in the forest)\n",
    "\n",
    "Let’s understand how algorithm works for a random forest model:\n",
    "\n",
    "1)\tJust like in bagging, different samples are collected from the training dataset using bootstraping.\n",
    "\n",
    "2)\tOn each sample we train our tree model and we allow the trees to grow with high depths. \n",
    "\n",
    "3)\tOnce the trees are formed, prediction is made by the random forest by aggregating the predictions of all the model.  For regression model, the mean of all the predictions is the final prediction and for classification mode, the mode of all the predictions is considered the final predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10)\tWhat is the difference between Bagging and Random forest? Why do we use Random forest more commonly than Bagging?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference with in random forest is how the trees are formed. In bootstraping we allow all the sample data to be used for splitting the nodes but not   with random forests.  When building a decision tree, each time a split is to happen, a random sample of ‘m’ predictors are chosen from the total ‘p’ predictors. Only those ‘m’ predictors are allowed to be used for the split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11)\tWhat is feature sampling?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uppose in those ‘p’ predictors, 1 predictor is very strong. Now each sample this predictor will remain the strongest. So, whenever trees will be built for these sampled data, this predictor will be chosen by all the trees for splitting and thus will result in similar kind of tree formation for each bootstrap model. This introduces correaltion in the dataset and averaging correalted dataset results do not lead low variance. That’s why in random forest the choice for selecting node for split is limited and it introduces randomness in the formation of the trees as well.\n",
    "    Most of the predictors are not allowed to be considered for split.\n",
    "    Generally, value of ‘m’ is taken as m ≈√p , where ‘p’ is the number of predictors in the sample.\n",
    "\n",
    "    When m=p , the random forest model becomes bagging model.   \n",
    "              \n",
    "    *This method is also referred as “Feature Sampling”\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12)\tHow prediction is made in Random Forest?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the given dataset different samples are created by bootstrapping and these samples are used to train different decision trees. Once the training is complete, prediction is made using all the different models.\n",
    "Predicting Outcome\n",
    " \n",
    "Random forest makes the prediction by taking the mode of all the predictions made by all the models, since this is the case of classification. This process is also known as “Majority voting”. We can also use prediction probability to make the final prediction. We can use the predict_proba method, which will predict a probability from 0 to 1 that a given class is the right one for a row. For a problem with output being only 0 and 1, we'll get a matrix with as many rows as there is in the data and 2 columns. predict_proba will return something like this:\n",
    " \n",
    "Each row corresponds to a prediction. The first column is the probability that the prediction is a 0, the second column is the probability that the prediction is a 1. Each row adds up to 1.\n",
    "If we just take the second column, we get the average value that the classifier would predict for that row. If there's a .9 probability that the correct classification is 1, we can use the .9 as the value the classifier is predicting. This will give us a continuous output in a single vector instead of just 0 or 1. We can then add all of the vectors we get through this method together and divide by the number of vectors to get the mean prediction by all the members of the ensemble. We can then round off to get 0 or 1 predictions. Similarly, in case of regression Random forest makes the prediction by taking the mean of all the predictions made by different models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**13)\tWhen should we not use Random forest model?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, the Random Forest cannot be applied to the following data types:\n",
    "\n",
    "images\n",
    "audio\n",
    "text (after preprocessing data will be sparse and RF doesn't work well with sparse data)\n",
    "\n",
    "For tabular data type, it is always good to check Random Forest because:\n",
    "\n",
    "it requires less data preparation and preprocessing than Neural Networks or SVMs. For example, you don't need to do feature scaling.\n",
    "For Random Forest training you can just use default parameters and set the number of trees (the more trees in RF the better). When you compare Random Forest to Neural Networks, the training is very easy (don't need to define architecture, or tune training algorithm). Random Forest is easier to train than Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**14)\tWhat is Stacking?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking is a type of ensemble technique which combines the predictions of two or more models, also called base models, and use the combination as the input for a new model (meta-model) i.e. a new model is trained on the predictions of the base models. \n",
    "\n",
    "Suppose you have a classification problem and you can use several models like logistic regression, SVM, KNN, Random forest etc. The idea is to use few models like KNN, SVM as the base model and make predictions using these models. Now the predictions made by these models are used as an input feature for Random forest to train on and give prediction.\n",
    "\n",
    "Stacking, just like other ensemble techniques, tries to improve the accuracy of a model by using predictions of not so good models and then using those predictions as an input feature for a better model.\n",
    "\n",
    "Stacking can be multilevel e.g. using base models as level 1 then passing the predictions into another set of sub-base models at level 2 and so on. Then at the end using meta-model/models which take predictions of the last sub base models as input and does prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**15)\tExplain the working behind Stacking.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand more by looking at the steps involved for stacking:\n",
    "\n",
    "*\tSplit the dataset into a training set and a holdout set. We can use k-fold validation for seleting different set of validation sets.\n",
    "\n",
    "   Generally, we do a 50-50 split of the training set and the hold out set. \n",
    "   \n",
    "   training set = x1,y1\n",
    "   hold out set = x2, y2\n",
    "\n",
    "*\tSplit the training set again into training and test dataset e.g. x1_train, y1_train, x1_test, y1_test\n",
    "\n",
    "*\tTrain all the base models on training set  x1_train, y1_train.\n",
    "\n",
    "*\tAfter training is done, get the predictions of all the base models on the validation set x2. \n",
    "\n",
    "*\tStack all these predictions together (you can also take an average of all the predictions or probability prediction) as it will be used as input feature for the meta_model.\n",
    "\n",
    "*\tAgain, get the prediction for all the base models on the test set i.e. x1_test \n",
    "\n",
    "*\tAgain, stack all these predictions together (you can also take an average of all the predictions or probability prediction) as it will be used as the prediction dataset for the meta_model.\n",
    "\n",
    "*\tUse the stacked data from step 5 as the input feature for meta_model and validation set y2 as the target variable and train the model on these data.\n",
    "\n",
    "*\tOnce, the training is done check the accuracy of meta_model by using data from step 7 for prediction and y1_test for evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
