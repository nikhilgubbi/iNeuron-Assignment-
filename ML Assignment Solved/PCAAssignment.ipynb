{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.\tExplain the curse of dimensionality.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High dimensionality makes clustering hard, because having lots of dimensions means that everything is “far away” from each other.For example, to cover a fraction of the volume of the data we need to capture a very wide range for each variable as the number of variables increases.All samples are close to the edge of the sample. And this is a bad news because prediction is much more difficult near the edges of the training sample.The sampling density decreases exponentially as p increases and hence the data becomes much more sparse without significantly more data.We should conduct PCA to reduce dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.\tWhat is a dimensionality reduction technique?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction is a feature selection technique using which we reduce the number of features to be used for making a model without losing a significant amount of information compared to the original dataset. In other words, a dimensionality reduction technique projects a data of higher dimension to a lower-dimensional subspace.\n",
    "\n",
    "**When to use Dimensionality Reduction?**\n",
    "Dimensionality reduction shall be used before feeding the data to a machine learning algorithm to achieve the following:\n",
    "\n",
    "* It reduces the size of the space in which the distances are calculated, thereby improving machine learning algorithm performance.\n",
    "* It reduces the degrees of freedom for our dataset avoiding chances of overfitting\n",
    "* Reducing the dimensionality using dimensionality reduction techniques can simplify the dataset facilitating a better description, visualisation, and insight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.\tExplain PCA. What are the principal components?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principal component analysis is an unsupervised machine learning algorithm used for feature selection using dimensionality reduction techniques. As the name suggests, it finds out the principal components from the data. PCA transforms and fits the data from a higher-dimensional space to a new, lower-dimensional subspace This results into an entirely new coordinate system of the points where the first axis corresponds to the first principal component that explains the most variance in the data.\n",
    "\n",
    "Principal components are the derived features which explain the maximum variance in the data. The first principal component explains the most variance, the 2nd a bit less and so on. Each of the new dimensions found using PCA is a linear combination of the old features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.\tWhat is explained variance ratio?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fraction of variance explained by a principal component is the ratio between the variance of that principal component and the total variance.For several principal components, add up their variances and divide by the total variance.\n",
    "\n",
    "It represents the amount of variance each principal component is able to explain.\n",
    "\n",
    "For example, suppose if the square of distances of all the points from the origin that lie on PC1 is 50 and for the points on PC2 it’s 5.\n",
    "\n",
    "EVR of PC1=$\\frac{Distance of PC1 points}{( Distance of PC1 points+ Distance of PC2 points)}=\\frac{50}{55}=0.91 $\n",
    "\n",
    "EVR of PC2=$\\frac{Distance of PC2 points}{( Distance of PC1 points+ Distance of PC2 points)}=\\frac{5}{55}=0.09 $\n",
    "\n",
    "\n",
    "Thus PC1 explains 91% of the variance of data. Whereas, PC2 only explains 9% of the variance. Hence we can use only PC1 as the input for our model as it explains the majority of the variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.\tWhat is a scree plot?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scree plots are the graphs that convey how much variance is explained by corresponding Principal components. \n",
    "\n",
    "A scree plot shows the eigenvalues on the y-axis and the number of factors on the x-axis. It always displays a downward curve.\n",
    "\n",
    "The point where the slope of the curve is clearly leveling off (the “elbow) indicates the number of factors that should be generated by the analysis.\n",
    "\n",
    "Unfortunately, both criteria sometimes yield an unreasonably high number of factors. In the above example, a cut-off of an eigenvalue ≥1 would give you seven factors. And the scree plot suggests either three or five factors due to the way the slope levels off twice.\n",
    "\n",
    "It is important to keep in mind that one of the reasons for running a factor analysis is to reduce the large number of variables that describe a complex concept such as socioeconomic status to a few interpretable latent variables (=factor). In other words, we would like to find a smaller number of interpretable factors that explain the maximum amount variability in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.\tHow is the optimum number of principal components obtained?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above questions are answered using the *explained variance ratio*. It represents the amount of variance each principal component is able to explain.\n",
    "\n",
    "For example, suppose if the square of distances of all the points from the origin that lie on PC1 is 50 and for the points on PC2 it’s 5.\n",
    "\n",
    "EVR of PC1=$\\frac{Distance of PC1 points}{( Distance of PC1 points+ Distance of PC2 points)}=\\frac{50}{55}=0.91 $\n",
    "\n",
    "EVR of PC2=$\\frac{Distance of PC2 points}{( Distance of PC1 points+ Distance of PC2 points)}=\\frac{5}{55}=0.09 $\n",
    "\n",
    "\n",
    "Thus PC1 explains 91% of the variance of data. Whereas, PC2 only explains 9% of the variance. Hence we can use only PC1 as the input for our model as it explains the majority of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.\tWhat is covariance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance is the measure of how a variable changes or varies and co means together. Hence, covariance is the measure of how two variables change together. \n",
    "\n",
    "If the covariance is high, it means that the variables are highly correlated and change in one results in a change in the other one too. Generally, we avoid using highly correlated variables in building a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.\tWhat is the transpose of a matrix?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transpose of a matrix is a new matrix whose rows are the columns of the original. (This makes the columns of the new matrix the rows of the original). Here is a matrix and its transpose:\n",
    "\n",
    "Matrix Transpose\n",
    "The superscript \"T\" means \"transpose\".\n",
    "\n",
    "Another way to look at the transpose is that the element at row r column c in the original is placed at row c column r of the transpose. The element arc of the original matrix becomes element acr in the transposed matrix.\n",
    "\n",
    "Usually we will work with square matrices, and it is usually square matrices that will be transposed. However, non-square matrices can be transposed, as well:\n",
    "\n",
    "Rectangular Matrix, transposed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9.\tWhat is an Eigen value and Eigen Vector?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **eigenvector v** of a linear transformation **T** is a nonzero vector that, when **T** is applied to it, does not change direction. Applying __T__ to the eigenvector only scales the eigenvector by the scalar value λ, called an **eigenvalue**. This condition can be written as the equation\n",
    "\n",
    "Eigenvalues and eigenvectors allow us to \"reduce\" a linear operation to separate, simpler, problems. For example, if a stress is applied to a \"plastic\" solid, the deformation can be dissected into \"principle directions\"- those directions in which the deformation is greatest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10.\tWhy the Principal Components are orthogonal?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea of principal component analysis (PCA) is to reduce the dimensionality of a data set consisting of many variables correlated with each other, either heavily or lightly, while retaining the variation present in the dataset, up to the maximum extent. The same is done by transforming the variables to a new set of variables, which are known as the principal components (or simply, the PCs) and are orthogonal, ordered such that the retention of variation present in the original variables decreases as we move down in the order. So, in this way, the 1st principal component retains maximum variation that was present in the original components. The principal components are the eigenvectors of a covariance matrix, and hence they are orthogonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**11.\tExplain the Eigen Decomposition approach.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to [Wikipedia article on PCA](https://en.m.wikipedia.org/wiki/Principal_component_analysis), *\"PCA can be done by eigenvalue decomposition of a data covariance (or correlation) matrix or singular value decomposition of a data matrix.\"* The second approach has already been discussed above. Let's discuss the first approach now.\n",
    "\n",
    "$\\Sigma$ is a real, symmetric matrix; thus, it has \n",
    "\n",
    "1) real eigenvalues, and\n",
    "\n",
    "2) orthogonal eigenvectors.\n",
    "\n",
    "$$\n",
    "{\\displaystyle T(\\mathbf {v} )=\\lambda \\mathbf {v} ,} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.\tHow can PCA be used for data compression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components\n",
    "\n",
    "As mentioned, PCA can be used for a sort of data compression as well. Using a smaller value of ``n_components`` allows you to represent a higher dimensional point as a sum of just a few principal component vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**13.\tDiscuss the pros and cons of PCA.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros of PCA:**\n",
    "    \n",
    "Correlated features are removed.\n",
    "\n",
    "Model training time is reduced.\n",
    "\n",
    "Overfitting is reduced.\n",
    "\n",
    "Helps in better visualizations\n",
    "\n",
    "Ability to handle noise\n",
    "\n",
    "**Cons of PCA**\n",
    "\n",
    "The resultant principal components are less interpretable than the original data\n",
    "\n",
    "Can lead to information loss if the explained variance threshold is not considered appropriately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
